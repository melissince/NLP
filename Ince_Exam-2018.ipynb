
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "###### Starting point:\n",
    "\n",
    "In the folder \"Dataset\" there is a single .tsv file. Each line of this file consists of a newspaper article.\n",
    "\n",
    "###### Task:\n",
    "\n",
    "Read each line, one after the other, and save all article-content in a single corpus list and all other metadata information (title, date, topic) in other lists. Remember to skip the header row and do not consider articles shorter than 6 *words*. If you want, you can use different data structures to organize the dataset (e.g., pandas frameworks) - if you do, document all the steps.\n",
    "\n",
    "Hint 1: The file is a .tsv, so each line is \"tab\" separated. You need to split over tabs to identify each column.\n",
    "\n",
    "Hint 2: The first line will tell you the structure of the file (or you could also open it and look at it)\n",
    "\n",
    "Hint 3: I said \"words\", not token\n",
    "\n",
    "Hint 4: if you skip an article, remember to also skip its metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = []\n",
    "date = []\n",
    "title=[]\n",
    "topic=[]\n",
    "article=[]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/Users/melisince/Desktop/text analytics/Comp-Text-Analysis-2018-19-master/datasets/dataset.tsv',\n",
    "                         sep='\\t', header=0) #I downloaded the tsv file as a pandas dataframe and I informed python that the file is tab seperated by using sep='\\t'\n",
    "#so now it is going to be easy to extract the specific columns from the dataframe by their name.\n",
    "data.columns=['date','title','topic','content'] #here I renamed the name of the columns because I was not sure if columns are called as they are written on the tsv file\n",
    "\n",
    "data=data.iloc[1:] #above header=0 did not exclude the header row, so I tried this iloc. \n",
    "\n",
    "wordcount=data['content'].str.split().str.len() #the words are splitted by the white spaces, however the numbers and special characters like \n",
    "#twitter links or photo links for the articles are counted as a word here. \n",
    "\n",
    "data['wordcount']=wordcount #Here I created a new column called 'wordcount' and put the word counting of the content column into the wordcount column\n",
    "\n",
    "data=pd.DataFrame(data[data.wordcount>6]) #dropped the articles which have wordcounts less than 6. from now on, contents with longer than 6 words are going to be in article corpus.\n",
    "\n",
    "#here I assign each column to its so called list.\n",
    "article=data.content.tolist() \n",
    "title=data.title.tolist()\n",
    "date=data.date.tolist()\n",
    "topic=data.topic.tolist()\n",
    "\n",
    "print(len(article)) #I checked the length of the list in order to be sure that they are equal to the length of content column of the tsv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Exercise 2:",
    "\n",
    "\n",
    "\n",
    "###### Starting point:\n",
    "\n",
    "Take your corpus and topic lists.\n",
    "\n",
    "Train 1 type of topic classifier of your choice (among naive-bayes, knn and NearestCentroid) using pre-trained word embeddings to generate the feature-vector.\n",
    "\n",
    "###### Task:\n",
    "\n",
    "Compare the performance (always using 10 fold cross validation!) of at least 2 different pre-processing steps (for instance lemmatization vs non-lemmatization, stopword removal or not, only using NER instead of using lemmas, only using nouns, only using name of organizations). \n",
    "\n",
    "Present here: \n",
    "1. The two different text_embedding functions you used (example, a pipeline where you use lemmatization and one without lemmatization); \n",
    "2. The cross-validation experiment, showing the different outputs in terms of precision, recall and F1\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /anaconda3/lib/python3.6/site-packages (3.6.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.14.3)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: bz2file in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (1.9.25)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.25 in /anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.25)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.25->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.25->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#classification task. classification styles, NBC or Nearest neighbor... please submit different exams he gets out from here. \n",
    "#if you decide filter out stop words then keep them in once and filter out in the other and compare the results. \n",
    "#two different functions and two different classification steps. \n",
    "\n",
    "\n",
    "!pip install gensim #gensim is a library famous for word embeddings and semantics\n",
    "\n",
    "import gensim, logging\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/melisince/Desktop/text analytics/Comp-Text-Analysis-2018-19-master/resources/small-embeddings.txt', binary=False)\n",
    "\n",
    "import nltk, string #nltk is for the tokenization and string is for extracting the punctuation.\n",
    "from nltk.corpus import stopwords #I want to remove the stopwords which makes it easier for the function to clean the articles.. \n",
    "from nltk.stem.wordnet import WordNetLemmatizer #for the lemmatization. \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) #list of punctuations to remove them\n",
    "stop_word_list = stopwords.words('english') #extracting the list of stopwords in english.\n",
    "\n",
    "\n",
    "def text_embedding_first(text):\n",
    "    \n",
    "    text = nltk.word_tokenize(text)#tokenization\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]#removing characters that are not words in english and removing the punctuations. \n",
    "    text = [token for token in text if token not in stop_word_list] #keeping the words which are not in stop words lists\n",
    "    text = nltk.pos_tag(text) #pos tagging for lemmatization \n",
    "    text = [wordnet_lemmatizer.lemmatize(token,\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token) for token,pos in text] #lemmatization \n",
    "    article_embedd = [] #wcreating an empty list for the word embeddings.\n",
    "    #the for loop below will do the word embedding depending on the word2vec model\n",
    "    for word in text: \n",
    "            try:\n",
    "                embed_word = model[word] #model is the vocab of word embeddings where words are associated with word embeddings. \n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)] #average vectors of all words in the articles. this is the column wise average. \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "#creating first corpus for the text embedding with lemmatization and removed stopwords. \n",
    "corpusfirst = []\n",
    "\n",
    "for word in article:\n",
    "    emb_text = text_embedding_first(word) #word embeddings are created. \n",
    "    if len(emb_text) > 0: #take the articles with word embeddings bigger than zero.\n",
    "        corpusfirst.append(emb_text)\n",
    "    \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132\n"
     ]
    }
   ],
   "source": [
    "print(len(corpusfirst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /anaconda3/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.46795664693051303, 0.5093668023188644, 0.46282658000248905, None)\n",
      "(0.5367591117883951, 0.633158164418979, 0.5382985835212509, None)\n",
      "(0.5219512379861819, 0.5628610083468669, 0.5177576386818213, None)\n",
      "(0.5217586526715249, 0.5472886234112994, 0.5120570012318031, None)\n",
      "(0.497707712898046, 0.519427125233713, 0.48689110016789205, None)\n",
      "(0.5185278065756638, 0.5592686029817497, 0.5154939256848344, None)\n",
      "(0.5311040103819039, 0.5700821659225941, 0.5271547916898284, None)\n",
      "(0.47235656194173975, 0.5026398561766354, 0.4625424237652334, None)\n",
      "(0.5308772624177686, 0.6422728586426367, 0.5470754757419743, None)\n",
      "(0.4658153940415664, 0.5002168453304714, 0.46134215923385463, None)\n",
      " \n",
      "0.5031439679720981\n",
      "0.5064814397633304\n",
      "0.5064814397633304\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# I use np array as they are more efficient\n",
    "X = np.array(corpusfirst)\n",
    "y = np.array(topic)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "final_f1first = [] \n",
    "final_precisionfirst=[]\n",
    "final_recallfirst=[]\n",
    "\n",
    "#we set that we do 10 fold cross validation\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    classifier = GaussianNB().fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\")) #if you are only interested in positive values, macro average between the two classes.\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    precision_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[0]\n",
    "    recall_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[1]\n",
    "    final_f1first.append(f1_score)\n",
    "    final_precisionfirst.append(precision_score)\n",
    "    final_recallfirst.append(precision_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1first)/len(final_f1first))\n",
    "print (sum(final_precisionfirst)/len(final_precisionfirst))\n",
    "print (sum(final_recallfirst)/len(final_recallfirst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the second text preprocessing function without lemmatization\n",
    "def text_embedding_second(text):\n",
    "    \n",
    "    #here I tokenized the each word for the preprocessign the articles. \n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]#removing characters that are not words in english and removing the punctuation basically. \n",
    "    text = [token for token in text if token not in stop_word_list] #keeping the words which are not in stop words lists\n",
    "    article_embedd = []\n",
    "\n",
    "    for word in text: \n",
    "            try:\n",
    "                embed_word = model[word] #model is the vocab of word embeddings where words are associated with word embeddings. \n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)] #average vectors of all words. this is the column wise average. this represent the meaning of your document.\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "corpussecond = []\n",
    "\n",
    "for word in article:\n",
    "    emb_text = text_embedding_second(word) \n",
    "    if len(emb_text) > 0:\n",
    "        corpussecond.append(emb_text)\n",
    "    \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.45829191344944253, 0.4824107289001071, 0.44959298570241707, None)\n",
      "(0.552694806992145, 0.6443559271206698, 0.5600740067659055, None)\n",
      "(0.48892609340295556, 0.5271607512308053, 0.48560925191903853, None)\n",
      "(0.5310848279561673, 0.5677506572274106, 0.5258038692277606, None)\n",
      "(0.5319257884470358, 0.5575115866017448, 0.5231440540139846, None)\n",
      "(0.5285229599952689, 0.5633850339442689, 0.5204597372707128, None)\n",
      "(0.5433299032745053, 0.5637408242611296, 0.5256976496086351, None)\n",
      "(0.6232695075744785, 0.6606893103160637, 0.616081862858994, None)\n",
      "(0.5095074657392733, 0.5498375614868621, 0.5062345501431289, None)\n",
      "(0.5323496291129115, 0.5645023203439636, 0.5274146907581966, None)\n",
      " \n",
      "0.5240112658268774\n",
      "0.5299902895944184\n",
      "0.5681344701433027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# I use np array as they are more efficient\n",
    "X = np.array(corpussecond)\n",
    "y = np.array(topic)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "final_f1second = []\n",
    "final_precisionsecond=[]\n",
    "final_recallsecond=[]\n",
    "#we set that we do 10 fold cross validation\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    classifier = GaussianNB().fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\")) #if you are only interested in positive values, macro average between the two classes.\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    precision= precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[0]\n",
    "    recall=precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[1]\n",
    "    final_f1second.append(f1_score)\n",
    "    final_precisionsecond.append(precision)\n",
    "    final_recallsecond.append(recall)\n",
    "print (\" \")\n",
    "print (sum(final_f1second)/len(final_f1second)) #the average of fscore\n",
    "print(sum(final_precisionsecond)/len(final_precisionsecond)) #the average of precision\n",
    "print(sum(final_recallsecond)/len(final_recallsecond))#the average of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the precision, recall and f1 scores are changing between two texts which shows that text preprocessing matters. \n",
    "#Without lemmatization in the second text preprocessing, the precision, f1 and recall scores shows better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Exercises 3: 15 points\n",
    "\n",
    "Take your corpus list.\n",
    "\n",
    "###### Task:\n",
    "\n",
    "Lemmatize the corpus, remove stopwords and lemmas shorter than 5 characters. Then convert each article in a document embedding.\n",
    "\n",
    "Group documents in 10 clusters using k-means. \n",
    "\n",
    "Counts how many documents you have, for each cluster.\n",
    "\n",
    "\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_word_list = stopwords.words('english') \n",
    "\n",
    "def text_embedding_third(text):\n",
    "    \n",
    "    text = nltk.word_tokenize(text)#here I tokenized the each word for the preprocessign the articles.\n",
    "    text = [token for token in text if token not in stop_word_list] #keeping the words which are not in stop words lists\n",
    "    text = nltk.pos_tag(text)\n",
    "    text = [wordnet_lemmatizer.lemmatize(token,\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token) for token,pos in text] \n",
    "    lemma_article=[] #here the list will contain lemma longer than 5 characters. \n",
    "    for lemma in text:\n",
    "        if len(lemma)>5:\n",
    "            lemma_article.append(lemma)\n",
    "    article_embedd = [] #I want to use our vocabulary of word embeddings\n",
    "# for each word in the article, I take the embeddings. I have a gigantic list of word embeddings of each word. I simply do a column wise average. then I have your doc embedding.\n",
    "    for word in lemma_article: \n",
    "            try:\n",
    "                embed_word = model[word] #model is the vocab of word embeddings where words are associated with word embeddings. \n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)] #average vectors of all words. this is the column wise average. \n",
    "    return avg\n",
    "\n",
    "corpusthird = []\n",
    "\n",
    "for word in article:\n",
    "    emb_text = text_embedding_third(word) #this function does all the things we have learnt throughout the lecture. in the exam, when you do text preprocessing write a function that takes your string into a vector.\n",
    "    if len(emb_text) > 0:\n",
    "        corpusthird.append(emb_text)\n",
    "    \n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11121\n"
     ]
    }
   ],
   "source": [
    "print(len(corpusthird))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #here is the library for KMeans. \n",
    "import numpy as np\n",
    "k = 10 #setting the number of cluster\n",
    "x = np.array(corpusthird)\n",
    "model = KMeans(k, init='k-means++', max_iter=100, n_init=1).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 1890, 9: 1715, 2: 1459, 0: 1234, 8: 1079, 3: 1002, 4: 864, 6: 729, 1: 643, 7: 506})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter,defaultdict #counting the documents in each cluster. \n",
    "print(Counter(model.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 4:\n",
    "\n",
    "Take your corpus list.\n",
    "\n",
    "###### Task:\n",
    "\n",
    "Lemmatize the corpus and keep only the nouns.\n",
    "\n",
    "Run LDA topic models with 50 topics.\n",
    "\n",
    "Feed the results into pyLDAvis to visualize them: https://github.com/bmabey/pyLDAvis/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the pipeline for the lemmatization of the nouns.\n",
    "def text_lemma(text):\n",
    "    tokentext = nltk.word_tokenize(text)\n",
    "    \n",
    "    cleantext = [token for token in tokentext if token not in exclude and token.isalpha()]\n",
    "\n",
    "    pos_text = nltk.pos_tag(cleantext)\n",
    "    \n",
    "    nouns= [] #the list of the nouns.\n",
    "    for word,pos in pos_text:\n",
    "        if (pos==\"NN\" or pos == \"NNP\" or pos==\"NNS\" or pos==\"NNPS\"):\n",
    "            nouns.append(word)\n",
    "    lemmatext = [wordnet_lemmatizer.lemmatize(token) for token in nouns]\n",
    "    \n",
    "    return(lemmatext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmano=[]#the final list that I will use in the LDA model dictionary\n",
    "for word in article:\n",
    "    lemmanouns=text_lemma(word)\n",
    "    lemmano.append(lemmanouns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['READ', 'MORE', 'plane', 'football', 'team', 'crash', 'Colombia', 'report', 'Ronaldinho', 'Argentina', 'star', 'Juan', 'Roman', 'Riquelme', 'club', 'Football', 'Ronaldinho', 'Juan', 'Roman', 'Riquelme', 'retirement', 'play', 'Chapecoensehttps', 'Ronaldinho', 'honor', 'playing', 'career', 'World', 'Cup', 'Riquelme', 'time', 'Argentina', 'country', 'player', 'year', 's', 'brother', 'agent', 'Roberto', 'Assis', 'Ronaldinho', 'club', 'way', 'discussion', 'place', 'time', 'Assis', 'Globo', 'Esporte', 'Later', 'contact', 'Ronaldinho', 'profile', 'guy', 'moment', 'family', 'expectation', 'Brazilians', 'plane', 'crash', 'people', 'club', 's', 'player', 'A', 'player', 'Danilo', 'hospital', 'wreckage', 'FIFA', 'president', 'Gianni', 'Infantino', 'funeral', 'Chapecoense', 'player', 'Friday', 'trip', 'Australia', 'woman', 'World', 'Cup', 'crash', 'world', 'UEFA', 'president', 'Aleksander', 'Ceferin', 'condolence', 'week', 'Champions', 'League', 'Europa', 'League', 'game', 's', 'silence', 'player', 'armband', 'READ', 'MORE', 'Chapecoense', 'plane', 'crash', 'Crew', 'Brazilian', 'team', 's', 'flight', 'time', 'football', 'sympathy', 'Football', 'Confederation', 'CBF', 'CONMEBOL', 'family', 'victim', 'week', 'air', 'disaster', 'Ceferin', 'statement', 'tragedy', 'world', 'football', 'support']\n"
     ]
    }
   ],
   "source": [
    "print(lemmano[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models #the packages are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready!\n"
     ]
    }
   ],
   "source": [
    "#For each article in the corpus, creating a dictionary \n",
    "dictionary = corpora.Dictionary(lemmano) \n",
    "A = [dictionary.doc2bow(text) for text in lemmano]\n",
    "print(\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(A, num_topics=50, id2word = dictionary, iterations=1000) \n",
    "print (\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, '0.098*\"Trump\" + 0.019*\"Donald\" + 0.017*\"Clinton\" + 0.014*\"election\" + 0.014*\"candidate\" + 0.012*\"percent\" + 0.010*\"voter\" + 0.008*\"poll\" + 0.008*\"campaign\" + 0.008*\"Obama\" + 0.007*\"Hillary\" + 0.007*\"president\" + 0.007*\"people\" + 0.007*\"Republican\" + 0.007*\"protester\" + 0.007*\"New\" + 0.007*\"US\" + 0.006*\"state\" + 0.006*\"supporter\" + 0.005*\"York\" + 0.005*\"time\" + 0.005*\"rally\" + 0.005*\"vote\" + 0.005*\"s\" + 0.004*\"medium\" + 0.004*\"victory\" + 0.004*\"protest\" + 0.004*\"party\" + 0.004*\"MORE\" + 0.004*\"crowd\"')\n",
      "(32, '0.028*\"Russia\" + 0.027*\"company\" + 0.023*\"year\" + 0.022*\"deal\" + 0.019*\"project\" + 0.014*\"energy\" + 0.013*\"Ukraine\" + 0.013*\"sanction\" + 0.012*\"country\" + 0.012*\"contract\" + 0.011*\"percent\" + 0.011*\"gas\" + 0.010*\"agreement\" + 0.009*\"investment\" + 0.008*\"market\" + 0.008*\"s\" + 0.008*\"Moscow\" + 0.008*\"MORE\" + 0.008*\"business\" + 0.008*\"READ\" + 0.007*\"power\" + 0.007*\"plant\" + 0.006*\"Gazprom\" + 0.006*\"Russian\" + 0.006*\"economy\" + 0.006*\"construction\" + 0.006*\"Minister\" + 0.005*\"pipeline\" + 0.005*\"President\" + 0.005*\"export\"')\n",
      "(12, '0.010*\"tax\" + 0.009*\"s\" + 0.009*\"year\" + 0.008*\"election\" + 0.006*\"government\" + 0.006*\"US\" + 0.006*\"state\" + 0.006*\"people\" + 0.005*\"Party\" + 0.005*\"job\" + 0.005*\"voter\" + 0.005*\"party\" + 0.005*\"percent\" + 0.004*\"House\" + 0.004*\"law\" + 0.004*\"campaign\" + 0.004*\"vote\" + 0.004*\"Smith\" + 0.004*\"time\" + 0.003*\"policy\" + 0.003*\"leader\" + 0.003*\"money\" + 0.003*\"ballot\" + 0.003*\"Street\" + 0.003*\"candidate\" + 0.003*\"support\" + 0.003*\"Labour\" + 0.003*\"system\" + 0.003*\"http\" + 0.003*\"worker\"')\n",
      "(21, '0.019*\"Syria\" + 0.018*\"UN\" + 0.015*\"group\" + 0.013*\"Russia\" + 0.009*\"US\" + 0.009*\"opposition\" + 0.009*\"Aleppo\" + 0.008*\"State\" + 0.008*\"Council\" + 0.008*\"terrorist\" + 0.008*\"ceasefire\" + 0.008*\"talk\" + 0.007*\"people\" + 0.007*\"peace\" + 0.007*\"attack\" + 0.007*\"Security\" + 0.007*\"force\" + 0.006*\"Foreign\" + 0.006*\"Russian\" + 0.006*\"statement\" + 0.006*\"resolution\" + 0.006*\"Lavrov\" + 0.006*\"Ministry\" + 0.006*\"conflict\" + 0.006*\"government\" + 0.006*\"country\" + 0.005*\"time\" + 0.005*\"area\" + 0.005*\"Moscow\" + 0.005*\"situation\"')\n",
      "(23, '0.041*\"Israel\" + 0.024*\"bank\" + 0.021*\"Bank\" + 0.020*\"Israeli\" + 0.013*\"Google\" + 0.011*\"company\" + 0.011*\"settlement\" + 0.010*\"Palestinians\" + 0.010*\"currency\" + 0.009*\"year\" + 0.007*\"Gaza\" + 0.007*\"West\" + 0.007*\"market\" + 0.007*\"investor\" + 0.006*\"Palestine\" + 0.006*\"asset\" + 0.006*\"gold\" + 0.006*\"reserve\" + 0.005*\"fund\" + 0.005*\"US\" + 0.005*\"dollar\" + 0.005*\"business\" + 0.005*\"s\" + 0.005*\"country\" + 0.005*\"MORE\" + 0.005*\"READ\" + 0.004*\"territory\" + 0.004*\"stock\" + 0.004*\"Yahoo\" + 0.004*\"percent\"')\n",
      "(7, '0.029*\"aircraft\" + 0.018*\"Air\" + 0.018*\"fan\" + 0.016*\"jet\" + 0.014*\"plane\" + 0.013*\"Force\" + 0.012*\"France\" + 0.012*\"drone\" + 0.011*\"Paris\" + 0.011*\"http\" + 0.008*\"flight\" + 0.008*\"fighter\" + 0.008*\"Marseille\" + 0.007*\"Euro\" + 0.006*\"US\" + 0.006*\"aviation\" + 0.005*\"force\" + 0.005*\"Secret\" + 0.005*\"football\" + 0.005*\"s\" + 0.005*\"people\" + 0.004*\"strike\" + 0.004*\"security\" + 0.004*\"Aviation\" + 0.004*\"Manbij\" + 0.004*\"city\" + 0.004*\"target\" + 0.004*\"group\" + 0.004*\"year\" + 0.004*\"IS\"')\n",
      "(41, '0.074*\"Clinton\" + 0.029*\"Sanders\" + 0.026*\"email\" + 0.023*\"campaign\" + 0.023*\"Hillary\" + 0.010*\"Bernie\" + 0.010*\"DNC\" + 0.010*\"Podesta\" + 0.009*\"s\" + 0.009*\"WikiLeaks\" + 0.008*\"election\" + 0.008*\"Bill\" + 0.005*\"Wall\" + 0.005*\"server\" + 0.005*\"FBI\" + 0.005*\"state\" + 0.005*\"leak\" + 0.005*\"State\" + 0.005*\"candidate\" + 0.005*\"Foundation\" + 0.005*\"party\" + 0.005*\"Obama\" + 0.005*\"Street\" + 0.004*\"Assange\" + 0.004*\"medium\" + 0.004*\"document\" + 0.004*\"secretary\" + 0.004*\"people\" + 0.004*\"US\" + 0.004*\"National\"')\n",
      "(48, '0.013*\"police\" + 0.012*\"man\" + 0.012*\"child\" + 0.011*\"car\" + 0.009*\"student\" + 0.009*\"woman\" + 0.008*\"prison\" + 0.008*\"people\" + 0.008*\"player\" + 0.008*\"s\" + 0.007*\"family\" + 0.007*\"video\" + 0.007*\"school\" + 0.007*\"incident\" + 0.006*\"time\" + 0.006*\"County\" + 0.005*\"A\" + 0.005*\"mother\" + 0.005*\"football\" + 0.005*\"officer\" + 0.005*\"year\" + 0.005*\"girl\" + 0.005*\"son\" + 0.005*\"day\" + 0.004*\"men\" + 0.004*\"sex\" + 0.004*\"club\" + 0.004*\"home\" + 0.004*\"complaint\" + 0.004*\"driver\"')\n",
      "(38, '0.017*\"FIFA\" + 0.008*\"investigation\" + 0.008*\"robot\" + 0.007*\"s\" + 0.007*\"police\" + 0.006*\"Gaddafi\" + 0.006*\"time\" + 0.005*\"Dutch\" + 0.005*\"year\" + 0.005*\"Cup\" + 0.004*\"people\" + 0.004*\"Libya\" + 0.004*\"officer\" + 0.004*\"A\" + 0.004*\"device\" + 0.004*\"Point\" + 0.003*\"sister\" + 0.003*\"information\" + 0.003*\"African\" + 0.003*\"video\" + 0.003*\"NCA\" + 0.003*\"victim\" + 0.003*\"retailer\" + 0.003*\"evidence\" + 0.003*\"World\" + 0.003*\"country\" + 0.003*\"robbery\" + 0.003*\"tower\" + 0.003*\"report\" + 0.003*\"parking\"')\n",
      "(8, '0.021*\"US\" + 0.015*\"war\" + 0.014*\"government\" + 0.009*\"report\" + 0.009*\"people\" + 0.008*\"year\" + 0.007*\"Afghanistan\" + 0.007*\"UK\" + 0.006*\"Iraq\" + 0.006*\"country\" + 0.006*\"s\" + 0.006*\"right\" + 0.005*\"time\" + 0.005*\"Blair\" + 0.005*\"state\" + 0.005*\"Yemen\" + 0.005*\"crime\" + 0.004*\"Obama\" + 0.004*\"UN\" + 0.004*\"weapon\" + 0.004*\"group\" + 0.004*\"RT\" + 0.004*\"United\" + 0.004*\"official\" + 0.004*\"drone\" + 0.004*\"world\" + 0.003*\"law\" + 0.003*\"leader\" + 0.003*\"power\" + 0.003*\"conflict\"')\n",
      "(22, '0.038*\"Turkey\" + 0.029*\"attack\" + 0.018*\"people\" + 0.015*\"Erdogan\" + 0.012*\"Ankara\" + 0.012*\"IS\" + 0.010*\"Turkish\" + 0.010*\"group\" + 0.009*\"Islamic\" + 0.008*\"report\" + 0.008*\"State\" + 0.007*\"terrorist\" + 0.007*\"city\" + 0.007*\"bomb\" + 0.007*\"security\" + 0.007*\"Kurdish\" + 0.007*\"area\" + 0.005*\"government\" + 0.005*\"authority\" + 0.005*\"police\" + 0.005*\"Kurds\" + 0.005*\"Reuters\" + 0.005*\"ISIS\" + 0.005*\"Istanbul\" + 0.005*\"death\" + 0.005*\"country\" + 0.005*\"blast\" + 0.005*\"explosion\" + 0.005*\"place\" + 0.004*\"force\"')\n",
      "(5, '0.034*\"Russia\" + 0.012*\"Putin\" + 0.012*\"Germany\" + 0.011*\"READ\" + 0.010*\"s\" + 0.010*\"MORE\" + 0.010*\"party\" + 0.009*\"state\" + 0.008*\"election\" + 0.008*\"year\" + 0.008*\"country\" + 0.007*\"Ukraine\" + 0.007*\"Crimea\" + 0.007*\"Ministry\" + 0.007*\"Merkel\" + 0.007*\"head\" + 0.006*\"Russian\" + 0.006*\"Moscow\" + 0.006*\"Europe\" + 0.006*\"Vladimir\" + 0.006*\"sanction\" + 0.006*\"law\" + 0.005*\"group\" + 0.005*\"official\" + 0.005*\"leader\" + 0.005*\"member\" + 0.005*\"agency\" + 0.005*\"President\" + 0.005*\"people\" + 0.004*\"policy\"')\n",
      "(43, '0.013*\"medium\" + 0.011*\"journalist\" + 0.009*\"court\" + 0.008*\"EU\" + 0.007*\"newspaper\" + 0.007*\"government\" + 0.007*\"police\" + 0.007*\"news\" + 0.006*\"freedom\" + 0.006*\"Turkey\" + 0.005*\"Brussels\" + 0.005*\"people\" + 0.005*\"attack\" + 0.005*\"s\" + 0.005*\"authority\" + 0.005*\"official\" + 0.005*\"report\" + 0.005*\"case\" + 0.004*\"Karlov\" + 0.004*\"time\" + 0.004*\"state\" + 0.004*\"year\" + 0.004*\"press\" + 0.004*\"country\" + 0.004*\"incident\" + 0.004*\"law\" + 0.004*\"boy\" + 0.004*\"Belgium\" + 0.004*\"Erdogan\" + 0.004*\"READ\"')\n",
      "(30, '0.013*\"India\" + 0.011*\"land\" + 0.008*\"bear\" + 0.008*\"hole\" + 0.008*\"Russia\" + 0.007*\"GIPHY\" + 0.006*\"s\" + 0.006*\"bridge\" + 0.006*\"time\" + 0.005*\"MORE\" + 0.005*\"READ\" + 0.005*\"universe\" + 0.005*\"year\" + 0.005*\"animal\" + 0.004*\"visitor\" + 0.004*\"world\" + 0.004*\"park\" + 0.004*\"Amri\" + 0.004*\"Victory\" + 0.004*\"Tate\" + 0.003*\"wave\" + 0.003*\"A\" + 0.003*\"camera\" + 0.003*\"selfie\" + 0.003*\"place\" + 0.003*\"Far\" + 0.003*\"people\" + 0.003*\"Road\" + 0.003*\"country\" + 0.003*\"way\"')\n",
      "(6, '0.021*\"Muslim\" + 0.015*\"Muslims\" + 0.013*\"Assange\" + 0.011*\"Islam\" + 0.009*\"US\" + 0.009*\"country\" + 0.008*\"mosque\" + 0.008*\"religion\" + 0.008*\"s\" + 0.007*\"Duterte\" + 0.007*\"right\" + 0.007*\"Sweden\" + 0.006*\"Islamic\" + 0.006*\"embassy\" + 0.006*\"president\" + 0.006*\"group\" + 0.005*\"woman\" + 0.005*\"charge\" + 0.005*\"year\" + 0.005*\"UK\" + 0.005*\"government\" + 0.005*\"statement\" + 0.004*\"UN\" + 0.004*\"Fox\" + 0.004*\"extradition\" + 0.004*\"Philippines\" + 0.004*\"Robinson\" + 0.004*\"Obama\" + 0.004*\"Christians\" + 0.004*\"Johnson\"')\n",
      "(11, '0.037*\"photo\" + 0.027*\"A\" + 0.025*\"PDT\" + 0.011*\"Twitter\" + 0.009*\"medium\" + 0.007*\"film\" + 0.007*\"picture\" + 0.006*\"image\" + 0.006*\"video\" + 0.005*\"Oct\" + 0.005*\"year\" + 0.004*\"account\" + 0.004*\"night\" + 0.004*\"post\" + 0.004*\"Instagram\" + 0.004*\"Bolt\" + 0.004*\"Sep\" + 0.004*\"woman\" + 0.004*\"world\" + 0.004*\"man\" + 0.003*\"movie\" + 0.003*\"studio\" + 0.003*\"flag\" + 0.003*\"artist\" + 0.003*\"Aug\" + 0.003*\"Vegas\" + 0.003*\"Tsipras\" + 0.003*\"time\" + 0.003*\"Jul\" + 0.003*\"people\"')\n",
      "(24, '0.087*\"Saudi\" + 0.055*\"Arabia\" + 0.055*\"Iran\" + 0.021*\"US\" + 0.016*\"oil\" + 0.015*\"Yemen\" + 0.014*\"Riyadh\" + 0.011*\"Tehran\" + 0.010*\"Saudis\" + 0.009*\"country\" + 0.008*\"deal\" + 0.006*\"s\" + 0.006*\"King\" + 0.005*\"meeting\" + 0.005*\"Bahrain\" + 0.005*\"day\" + 0.005*\"Minister\" + 0.005*\"producer\" + 0.005*\"production\" + 0.005*\"minister\" + 0.005*\"world\" + 0.004*\"execution\" + 0.004*\"Sudan\" + 0.004*\"sale\" + 0.004*\"President\" + 0.004*\"talk\" + 0.004*\"year\" + 0.004*\"market\" + 0.004*\"relation\" + 0.004*\"Shia\"')\n",
      "(28, '0.020*\"Cameron\" + 0.013*\"Farage\" + 0.012*\"Britain\" + 0.009*\"s\" + 0.009*\"leader\" + 0.008*\"Johnson\" + 0.008*\"PM\" + 0.008*\"people\" + 0.008*\"UK\" + 0.007*\"London\" + 0.007*\"EU\" + 0.007*\"Ireland\" + 0.007*\"David\" + 0.007*\"Boris\" + 0.007*\"Church\" + 0.007*\"year\" + 0.007*\"Pope\" + 0.006*\"minister\" + 0.005*\"country\" + 0.005*\"UKIP\" + 0.005*\"Prime\" + 0.005*\"England\" + 0.004*\"Minister\" + 0.004*\"Nigel\" + 0.004*\"party\" + 0.004*\"Francis\" + 0.004*\"time\" + 0.004*\"Theresa\" + 0.004*\"day\" + 0.004*\"politician\"')\n",
      "(33, '0.009*\"dog\" + 0.008*\"Chicago\" + 0.007*\"time\" + 0.007*\"s\" + 0.007*\"New\" + 0.006*\"A\" + 0.005*\"York\" + 0.005*\"vote\" + 0.005*\"year\" + 0.005*\"people\" + 0.004*\"guy\" + 0.004*\"t\" + 0.004*\"Messi\" + 0.004*\"PST\" + 0.004*\"day\" + 0.004*\"NHL\" + 0.004*\"Craig\" + 0.004*\"state\" + 0.003*\"spray\" + 0.003*\"player\" + 0.003*\"MMA\" + 0.003*\"lot\" + 0.003*\"Rubio\" + 0.003*\"Johnson\" + 0.003*\"Trump\" + 0.003*\"Florida\" + 0.003*\"Kasich\" + 0.003*\"Brown\" + 0.003*\"City\" + 0.003*\"event\"')\n",
      "(16, '0.019*\"NSA\" + 0.018*\"Snowden\" + 0.011*\"US\" + 0.010*\"Iraq\" + 0.009*\"Iraqi\" + 0.008*\"surveillance\" + 0.007*\"s\" + 0.007*\"Sanders\" + 0.006*\"force\" + 0.006*\"UK\" + 0.006*\"troop\" + 0.006*\"intelligence\" + 0.006*\"agency\" + 0.005*\"Intercept\" + 0.005*\"operation\" + 0.005*\"MoD\" + 0.004*\"http\" + 0.004*\"tire\" + 0.004*\"T\" + 0.004*\"document\" + 0.004*\"training\" + 0.004*\"campaign\" + 0.004*\"diamond\" + 0.004*\"war\" + 0.003*\"Defence\" + 0.003*\"revelation\" + 0.003*\"base\" + 0.003*\"personnel\" + 0.003*\"time\" + 0.003*\"RAF\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_words=30):\n",
    "    print (topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"forge\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-29a2d679b28f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Interactive visualisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip forge pyldavis install#it does not function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmano\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "#I have tried to import the package but it did not work in python 3. So i leave my codes here to show I have tried and I got an error.. \n",
    "\n",
    "## Interactive visualisation\n",
    "!pip forge pyldavis install#it does not function.\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, lemmano, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
